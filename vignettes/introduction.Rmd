---
title: '`muti` - An R package for computing mutual information'
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_depth: 3
    fig_caption: yes
---

[![DOI](https://zenodo.org/badge/85351399.svg)](https://zenodo.org/badge/latestdoi/85351399)

***

[__Mark D. Scheuerell__](https://faculty.washington.edu/scheuerl/)  
_Fish Ecology Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, Seattle, WA USA, mark.scheuerell@noaa.gov_

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(muti)
```

# Background

`muti` is an `R` package that computes the mutual information $(\mathrm{MI})$ between two discrete random variables. `muti` was developed with time series analysis in mind, but there is nothing tying the methods to a time index _per se_.

Mutual Information estimates the amount of information about one variable contained in another; it can be thought of as a nonparametric measure of the covariance between the two variables. $\mathrm{MI}$ is a function of entropy, which is the expected amount of information contained in a variable. The entropy of $X$, $\mathrm{H}(X)$, given its probability mass function, $p(X)$, is

$$
\begin{align}
\mathrm{H}(X) &= \mathrm{E}[-\log(p(X))]\\
 &= -\sum_{i=1}^{L} p(x_i) \log_bp(x_i),
\end{align}
$$

where $L$ is the length of the time series and $b$ is the base of the logarithm. `muti` uses base-2 logarithms for calculating the entropies, so $\mathrm{MI}$ measures information content in units of "bits". In cases where $p(x_i) = 0$, then $\mathrm{H}(X) = 0$.

The joint entropy of $X$ and $Y$ is

$$
\mathrm{H}(X,Y) = -\sum_{i=1}^{L} p(x_i,y_i) \log_b p(x_i,y_i).
$$

where $p(x_i,y_i)$ is the probability that $X = x_i$ and $Y = y_j$. The mutual information between $X$ and $Y$ is then

$$
\mathrm{MI}(X;Y) = \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y).
$$

One can normalize $\mathrm{MI}$ to the interval [0,1] as

$$
\mathrm{MI}^*(X;Y) = \frac{\mathrm{MI}(X;Y)}{\sqrt{\mathrm{H}(X)\mathrm{H}(Y)}}.
$$

# Using `muti`

__Input__. At a minimum `muti` requires two vectors of class `numeric` or `integer`. See `?muti` for all of the other function arguments.

__Output__. The output of `muti` is a data frame with the $\mathrm{MI}$ `MI_xy` and respective significance threshold value `MI_tv` at different lags. Note that a negative (positive) lag means _X_ leads (trails) _Y_. For example, if `length(x) == length(y) == TT`, then the $\mathrm{MI}$ in `x` and `y` at a lag of -1 would be based on `x[1:(TT-1)]` and `y[2:TT]`.

Additionally, `muti` produces a 3-panel plot of

1. the original data (top);
2. their symbolic or discretized form (middle);
3. $\mathrm{MI}$ values (solid line) and their associated threshold values (dashed line) at different lags (bottom).

The significance thresholds are based on a bootstrap of the original data. That process is relatively slow, so please be patient if asking for more than the default `mc=100` samples.

## Data discretization

`muti` computes $\mathrm{MI}$ based on 1 of 2 possible discretizations of the data in a vector `x`:

1. __Symbolic__. (Default) For `1 < i < length(x)`, `x[i]` is translated into 1 of 5 symbolic representations based on its value relative to `x[i-1]` and `x[i+1]`: "peak", "decreasing", "same", "trough", or "increasing". For example, the symbolic translation of the vector `c(1.1,2.1,3.3,1.2,3.1)` would be `c("increasing","peak","trough")`. For additional details, see [Cazelles (2004)](https://doi.org/10.1111/j.1461-0248.2004.00629.x).

2. __Binned__. Each datum is placed into 1 of `n` equally spaced bins as in a histogram. If the number of bins is not specified, then it is calculated according to Rice's Rule where `n = ceiling(2*length(x)^(1/3))`.

## Installation

You can install the development version using `devtools`.

```{r install, eval=FALSE}
if(!require("devtools")) {
  install.packages("devtools")
  library("devtools")
}
devtools::install_github("mdscheuerell/muti")
```

## Examples

### Ex 1: Real values as symbolic

Here's an example with significant information between two numeric vectors. Notice that none of the symbolic values are the "same".

```{r ex_1, fig.width=6, fig.height=8, warning=FALSE}
set.seed(123)
TT <- 30
x1 <- rnorm(TT)
y1 <- x1 + rnorm(TT)
muti(x1, y1)
```

### Ex 2: Integer values as symbolic

Here's an example with significant information between two integer vectors. Notice that in this case some of the symbolic values are the "same".

```{r ex_2, fig.width=6, fig.height=8, warning=FALSE}
x2 <- rpois(TT,4)
y2 <- x2 + sample(c(-1,1), TT, replace = TRUE)
muti(x2, y2)
```

### Ex 3: Real values as symbolic with normalized MI

Here are the same data as Ex 1 but with $\mathrm{MI}$ normalized to [0,1] (`normal = TRUE`). In this case the units are dimensionless.

```{r ex_3, fig.width=6, fig.height=8, warning=FALSE}
muti(x1, y1, normal = TRUE)
```

### Ex 4: Real values with binning

Here are the same data as Ex 1 but with regular binning instead of symbolic (`sym = FALSE`).

```{r ex_4, fig.width=6, fig.height=8, warning=FALSE, }
muti(x1, y1, sym = FALSE)
```

# Citation

Please cite the `muti` package as:

> Scheuerell, M. D. (2017) muti: An R package for computing mutual information. https://doi.org/10.5281/zenodo.439391

See `citation("muti")` for a BibTeX entry.
